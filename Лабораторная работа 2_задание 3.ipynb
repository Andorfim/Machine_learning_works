{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import warnings\n","import sklearn.exceptions\n","warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n","import pandas as pd\n","pd.options.mode.chained_assignment = None  # default='warn'\n","import nltk\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer \n","from sklearn.model_selection import train_test_split \n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB \n","from sklearn.metrics import * \n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"],"metadata":{"id":"VhRjEYUwt5YI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669202509637,"user_tz":-180,"elapsed":251,"user":{"displayName":"Данила Власов","userId":"10847520665763514044"}},"outputId":"3c65e968-795f-460e-bab4-fcfd1c7466e3"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["# Импортируем фрейм данных\n","data = pd.read_csv(\"dataset.csv\")\n","\n","# Оставляем только нужных нам исполнитеолей\n","df1 = data[data['cantorNome'] == 'david-bowie']\n","df2 = data[data['cantorNome'] == 'paul-mccartney']\n","data = pd.concat([df1, df2])\n","\n","# Оставляем только нужные нам колонки\n","columns = data[['cantorNome', 'letra']]\n","print(columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qyFc8O6FvLe-","executionInfo":{"status":"ok","timestamp":1669202509907,"user_tz":-180,"elapsed":3,"user":{"displayName":"Данила Власов","userId":"10847520665763514044"}},"outputId":"d020f998-3284-49e1-e5f7-8bf61e01a637"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["         cantorNome                                              letra\n","0       david-bowie  I, I will be king. And you, you will be queen....\n","1       david-bowie  Didn't know what time it was,. The lights were...\n","2       david-bowie  Ground control to Major Tom. Ground control to...\n","3       david-bowie  It's a god-awful small affair. To the girl wit...\n","4       david-bowie  I know when to go out. And when to stay in. Ge...\n","..              ...                                                ...\n","942  paul-mccartney  He's just a young boy looking for a way to fin...\n","943  paul-mccartney  How can I hope to reach your love. Help me to ...\n","944  paul-mccartney  I like it. Please don't take my heart away. It...\n","945  paul-mccartney  Yvonne is the one I´ve been counting on. She s...\n","946  paul-mccartney                                       Instrumental\n","\n","[947 rows x 2 columns]\n"]}]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"va8JuoFostxQ","executionInfo":{"status":"ok","timestamp":1669202511104,"user_tz":-180,"elapsed":1199,"user":{"displayName":"Данила Власов","userId":"10847520665763514044"}},"outputId":"6525489c-9037-4292-f80f-13917a755d25"},"outputs":[{"output_type":"stream","name":"stdout","text":["                precision    recall  f1-score   support\n","\n","   david-bowie       0.80      0.66      0.72       102\n","paul-mccartney       0.67      0.81      0.73        88\n","\n","      accuracy                           0.73       190\n","     macro avg       0.73      0.73      0.73       190\n","  weighted avg       0.74      0.73      0.73       190\n","\n"]}],"source":["# Приводим все к нижнему регистру\n","lowered = columns['letra'].str.lower()\n","columns['lowered'] = lowered\n","\n","# Токенезируем\n","tokenizer = RegexpTokenizer(r'\\w+')\n","tokened = columns.apply(lambda row: tokenizer.tokenize(row['lowered']), axis=1)\n","columns['tokened'] = tokened\n","\n","# Удаляем \"стоп слова\"\n","noise = stopwords.words('english')\n","withoutstop = columns['tokened'].apply(lambda x: [item for item in x if item not in noise])\n","without_stop = []\n","\n","for a in withoutstop:    \n","    without_stop.append(\", \".join(a))\n","columns['without_stop'] = without_stop\n","\n","# Делаем леммитизацию\n","lemmatizer = WordNetLemmatizer()\n","lemmatized = columns['without_stop'].apply(lambda x: [lemmatizer.lemmatize(x)])\n","lemma = []\n","\n","for a in lemmatized:    \n","    lemma.append(\", \".join(a))\n","columns['lemmatized'] = lemma\n","\n","# Разделяем фрейм на тренировочные и тестовые данные 80/20\n","x_train, x_test, y_train, y_test = train_test_split(columns.lemmatized, columns.cantorNome, train_size = 0.8)\n","\n","# Векторизируем тренировочную выборку\n","vectorizer = CountVectorizer(ngram_range=(1, 3))\n","vectorized_x_train = vectorizer.fit_transform(x_train)\n","\n","# Импортируем байесовский классификатор\n","clf = MultinomialNB()\n","clf.fit(vectorized_x_train, y_train)\n","\n","# Векторизируем тестовую выборку\n","vectorized_x_test = vectorizer.transform(x_test)\n","\n","# Покажем насколько хорошо модель предсказала тесты\n","preg = clf.predict(vectorized_x_test)\n","print(classification_report(y_test, preg))\n"]},{"cell_type":"code","source":["# Импортируем две песни, авторов которых мы хотим узнать\n","data = pd.read_csv(\"pesni.csv\")\n","\n","# Вся та же самая обработка фрейма\n","df1 = data[data['cantorNome'] == 'david-bowie'] \n","df2 = data[data['cantorNome'] == 'paul-mccartney']\n","data = pd.concat([df1, df2])\n","\n","columns = data[['cantorNome', 'letra']]\n","\n","lowered = columns['letra'].str.lower()\n","columns['lowered'] = lowered\n","\n","tokenizer = RegexpTokenizer(r'\\w+')\n","tokened = columns.apply(lambda row: tokenizer.tokenize(row['lowered']), axis=1)\n","columns['tokened'] = tokened\n","\n","withoutstop = columns['tokened'].apply(lambda x: [item for item in x if item not in noise])\n","without_stop = []\n","\n","for a in withoutstop:    \n","    without_stop.append(\", \".join(a))\n","columns['without_stop'] = without_stop\n","\n","lemmatizer = WordNetLemmatizer()\n","lemmatized = columns['without_stop'].apply(lambda x: [lemmatizer.lemmatize(x)])\n","lemma = []\n","\n","for a in lemmatized:    \n","    lemma.append(\", \".join(a))\n","\n","columns['lemmatized'] = lemma\n","\n","# X тесты - наши обработанные тексты двух песен, а Y тесты - исполнители, \n","# которых мы хотим предсказать, а потом сравнить предсказание с реальностью\n","x1_test = columns['lemmatized']\n","y1_test = columns['cantorNome']\n","\n","# Векторизируем наши тестовые песни\n","vectorized_x1_test = vectorizer.transform(x1_test)\n","\n","# Предсказываем авторов\n","pred = clf.predict(vectorized_x1_test)\n","print(*pred)   \n","\n","# Покажем что все правильно\n","print(classification_report(y1_test, pred))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T-XJE_520DIs","executionInfo":{"status":"ok","timestamp":1669202511104,"user_tz":-180,"elapsed":6,"user":{"displayName":"Данила Власов","userId":"10847520665763514044"}},"outputId":"c8bc6a92-697f-4b57-a5ff-55ef191646bc"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["david-bowie paul-mccartney\n","                precision    recall  f1-score   support\n","\n","   david-bowie       1.00      1.00      1.00         1\n","paul-mccartney       1.00      1.00      1.00         1\n","\n","      accuracy                           1.00         2\n","     macro avg       1.00      1.00      1.00         2\n","  weighted avg       1.00      1.00      1.00         2\n","\n"]}]}]}